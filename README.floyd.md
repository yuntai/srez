## Introduction
This project is an adaptation of [srez project](https://github.com/david-gpu/srez) for floydhub. You will need a [Floydhub](https://www.floydhub.com/)  account and have floyd-cli installed. See https://www.floydhub.com/welcome.

## Setup project
```bash
$ git clone https://github.com/yuntai/srez
$ cd srez
$ floyd init srez
```

## Training
The model is trained with [Large-scale CelebFaces Attributes (CelebA) Dataset](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html).
The dataset is available in Floydhub with ID (yo5Gx9CA8AkAo4XL54fgJX). With '--data <dataset id>', the data volume containing the dataset is mounted on `/input`.
The jpeg files are packed inside `/input/data/img_align_celebra.zip`. The zip file is first deflated into a local temporary directory and then used by the model training. 

To run training with the CelebA dataset available in Floydhub with 10 minutes of training time:
```bash
floyd run --env tensorflow-1.0 --gpu --data yo5Gx9CA8AkAo4XL54fgJX "python srez_main.py --run train --train_time 10 --dataset_zip /input/data/img_align_celeba.zip"
```

If you want to use your own image dataset, you need to specify `--dataset` instead of `--dataset_zip` pointing to the directory containing jpeg files. 

To check the progress,
```bash
floyd logs <RUN ID> -t
```

When the training is finished, you can check the output with 'floyd data' command. First, check the data id for the previous run with
```bash
floyd data status
```

```bash
floyd data output <data id>
```

In `train` directory, you can see the list of png files which are generated by the model training for every 100 batches.

From left to right, the first column is the 16x16 input image, the second one is what you would get from a standard bicubic interpolation, the third is the output generated by the neural net, and on the right is the ground truth. As the training preogress, you can find more plausible reconsturction is produced.

![Example output](srez_sample_output.png)

The project comes with a simple utility program to a generate video file combining these png files.
```bash
floyd run --env tensorflow-1.0 --data <data id> "python srez_main.py --run=demo --train_dir=/input/train"
```

You can check the produced video file in the output.

In data volume prepared in Floydhub (yo5Gx9CA8AkAo4XL54fgJX) does also contain the pnf files generated during training with 130,000,000 batches. 

To generate a video file,
```bash
floyd run --env tensorflow-1.0 --data yo5Gx9CA8AkAo4XL54fgJX "python srez_main.py --run=demo --train_dir=/input/train"
```

Here is the link to the movie file hosted in Floydhub.
[Training Video](https://www.floydhub.com/viewer/data/J6CtNCeRxMtzd2mi2jnnpT/CsU5XtPUokumiJX9Xs3qCb/demo1.mp4)

## Serve mode
You can now host this model as a REST API. This means you can send any image to this API as a HTTP request and its resolution will be enhanced.

Floyd [run](../commands/run.md) command has a `serve` mode. This will upload the files in the current directory and run a special command - 
`python app.py`. Floyd expects this file to contain the code to run a web server and listen on port `5000`. You can see the 
[app.py](https://github.com/yuntai/srez/blob/master/app.py) file in the repository. This file handles the 
incoming request, load the checkpoint from the pre-trained model (`/input/checkpoint/checkpoint_new.txt') and apply the generative operation.

The [Flask](http://flask.pocoo.org/) is used. The dependency is specified in `floyd_requirements.txt` file.

*Note that this feature is in preview mode and is not production ready yet*

```bash
$ floyd run --env tensorflow-1.0 --data yo5Gx9CA8AkAo4XL54fgJX --mode serve
Creating project run. Total upload size: 297.3KiB
Syncing code ...
RUN ID                  NAME                VERSION
----------------------  ----------------  ---------
niSuYzjT7hmqxAVP6M68bG  kobi2000/srez:48         48

Path to service endpoint: https://www.floydhub.com:8000/xtH75NqswHKuu3fVKWJ4La

To view logs enter:
    floyd logs niSuYzjT7hmqxAVP6M68bG

```

### Sending requests to the REST API

The app support two run modes. With `run=downsample` the image is downsampled by the factor of 4.

```bash
curl -o 000001_downsampled.jpg -F "file=@./000001.jpg" -F "run=downsample" https://www.floydhub.com:8000/xtH75NqswHKuu3fVKWJ4La
```

We are going to send this downscaled image back to the app to get a reconstructed image.
```bash
curl -o output.jpg -F "file=@./000001_downsampled.jpg" https://www.floydhub.com:8000/xtH75NqswHKuu3fVKWJ4La
```
